\begin{abstract}
As AI systems move into high-stakes domains---legal reasoning, medical diagnosis, financial decision-making---regulators and practitioners demand auditability: the ability to trace exactly what each step in a multi-step workflow saw and did. Yet current LLM-based workflows are fundamentally opaque. \textit{Context pollution}---the accumulation of information across reasoning steps---causes models to hallucinate and lose track of constraints, while implicit data flow makes it impossible to reconstruct what any given step actually received. We present \textbf{NormCode}, a semi-formal language that makes AI workflows \textit{auditable by construction}. Each inference operates in enforced data isolation with only explicitly passed inputs, eliminating cross-step contamination and ensuring that every intermediate state is inspectable. A strict separation between \textit{semantic operations} (LLM reasoning) and \textit{syntactic operations} (deterministic data flow) enables auditors to distinguish probabilistic inference from mechanical restructuring. The multi-format ecosystem (\texttt{.ncds}, \texttt{.ncd}, \texttt{.ncn}, \texttt{.ncdn}) allows developers, domain experts, and auditors to inspect the same plan in formats suited to their needs. A four-phase compilation pipeline transforms natural language intent into executable JSON repositories, while a visual Canvas App provides real-time graph visualization and breakpoint debugging. We validate the approach through 100\% accuracy on base-X addition and self-hosted execution of NormCode's own compiler, demonstrating that structured intermediate representations can bridge human intuition and machine rigor while maintaining full transparency.
\end{abstract}
